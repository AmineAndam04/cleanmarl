

## TO day
    - use a learning rate scheduler
    - revise how you update the target networks in parallel environments
    - Increase batch and mini-batch diversity
    - Weight decay
    - LBF and MADDPG may need n-step return updates

## Mistakes:
    - In QMIX: (1-batch_done[:,t])*q_tot_target should be t+1
    - the .device are wrong in all places
    - revise the epsilon in coma, did I compute it in the right place ??
    - correct these "ep_stats.extend([info["battle_won"] for info in ep_stat]) " look at mappo multienv for the correct 
    - Check that I reset h_eval in lstm evaluation loop

    - I should use done(t) not done(t+1)


- To check in every code:
    - Are devices used properly
    - Seeding
    - clip the gradients

- Things to do later:
    - separate the tbppt from normal training for lstm
    -  use n-step returns in vdn and qmix